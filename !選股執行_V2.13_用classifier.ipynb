{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 移除不必要的警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 獲取歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_finlab.data import Data\n",
    "\n",
    "data = Data()\n",
    "\n",
    "rev = data.get(\"當月營收\")\n",
    "\n",
    "close = data.get_adj(\"收盤價\")\n",
    "open_ = data.get_adj(\"開盤價\")\n",
    "high = data.get_adj(\"最高價\")\n",
    "low = data.get_adj(\"最低價\")\n",
    "vol = data.get(\"成交股數\")\n",
    "\n",
    "PB = data.get(\"股價淨值比\")\n",
    "pe = data.get(\"本益比\")\n",
    "DY = data.get(\"殖利率(%)\")\n",
    "\n",
    "rev.index = rev.index.shift(3, \"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 計算features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias(n):\n",
    "    return close / close.rolling(n, min_periods=1).mean()\n",
    "\n",
    "def acc(n):\n",
    "    return close.shift(n) / (close.shift(2*n) + close) * 2\n",
    "\n",
    "def rsv(n):\n",
    "    l = close.rolling(n, min_periods=1).min()\n",
    "    h = close.rolling(n, min_periods=1).max()\n",
    "    \n",
    "    return (close - l) / (h - l)\n",
    "\n",
    "def mom(n):\n",
    "    return (rev / rev.shift(1)).shift(n)\n",
    "\n",
    "#####上為原版資料###\n",
    "def yoy(n):\n",
    "    denominator = rev.shift(12 + n)\n",
    "    result = rev.shift(n) / denominator - 1\n",
    "    result[denominator < 0] = 0  # 如果分母为负，将对应的结果设置为0\n",
    "    return result\n",
    "\n",
    "def delta_yoy(n):\n",
    "    yoy_current = yoy(n)\n",
    "    yoy_previous = yoy(n + 1)\n",
    "    delta = yoy_current - yoy_previous\n",
    "    return delta\n",
    "\n",
    "def willr(n):\n",
    "    highest_high = high.rolling(n, min_periods=1).max()\n",
    "    lowest_low = low.rolling(n, min_periods=1).min()\n",
    "    willr = (highest_high - close) / (highest_high - lowest_low) * -100\n",
    "    return willr\n",
    "\n",
    "def linearreg_slope(n):\n",
    "    slope = (close - close.shift(n)) / n\n",
    "    return slope\n",
    "\n",
    "def adx(n):\n",
    "    true_range = high - low\n",
    "    true_range = true_range.fillna(0)\n",
    "    \n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    plus_dm = plus_dm.rolling(n).sum()\n",
    "    minus_dm = minus_dm.abs().rolling(n).sum()\n",
    "    \n",
    "    tr_sum = true_range.rolling(n).sum()\n",
    "    tr_sum[tr_sum == 0] = 0.0001\n",
    "    \n",
    "    plus_di = (plus_dm / tr_sum) * 100\n",
    "    minus_di = (minus_dm / tr_sum) * 100\n",
    "    \n",
    "    dx = ((plus_di - minus_di).abs() / (plus_di + minus_di).abs()) * 100\n",
    "    adx = dx.rolling(n).mean()\n",
    "    \n",
    "    return adx\n",
    "\n",
    "\n",
    "def adxr(n):\n",
    "    adx_value = adx(n)  # 先計算ADX值\n",
    "    adxr = adx_value.rolling(n).mean()  # 計算ADX值的平均值\n",
    "    return adxr\n",
    "\n",
    "def rsi(n):\n",
    "    diff = close.diff()\n",
    "    up = diff.where(diff > 0, 0)\n",
    "    down = -diff.where(diff < 0, 0)\n",
    "    avg_gain = up.rolling(n).mean()\n",
    "    avg_loss = down.rolling(n).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "def kd(n):\n",
    "    l = close.rolling(n).min()\n",
    "    h = close.rolling(n).max()\n",
    "    rsv = (close - l) / (h - l)\n",
    "    k = rsv.rolling(3).mean()\n",
    "    d = k.rolling(3).mean()\n",
    "    return k - d\n",
    "\n",
    "\n",
    "#---------\n",
    "\n",
    "def ma_ratio(n):\n",
    "    ma_short = close.rolling(n).mean()\n",
    "    ma_long = close.rolling(2 * n).mean()\n",
    "    return ma_short / ma_long\n",
    "\n",
    "def volume_change(n):\n",
    "    return vol.pct_change(n)\n",
    "\n",
    "def range_ratio(n):\n",
    "    range_high = high.rolling(n).max()\n",
    "    range_low = low.rolling(n).min()\n",
    "    return (range_high - range_low) / range_low\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "features = {\n",
    "    'mom{}'.format(n): mom(n) for n in range(1, 10)\n",
    "}\n",
    "\n",
    "features.update({\n",
    "    'bias_{}'.format(n): bias(n) for n in [5, 60, 120, 240]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'acc_{}'.format(n): acc(n) for n in [5, 10, 20, 60, 120, 240]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'rsv_{}'.format(n): rsv(n) for n in [60, 120, 240]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "    'mom{}'.format(n): mom(n) for n in range(1, 10)\n",
    "}\n",
    "\n",
    "features.update({\n",
    "    'bias_{}'.format(n): bias(n) for n in [5, 60, 120, 240]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'PB': PB,\n",
    "    'PE': pe,\n",
    "    'DY': DY\n",
    "})\n",
    "\n",
    "\n",
    "features.update({\n",
    "    'acc_{}'.format(n): acc(n) for n in [5, 10, 20, 60, 120, 240]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'rsv_{}'.format(n): rsv(n) for n in [60, 120, 240]\n",
    "})\n",
    "\n",
    "####上為原版資料###\n",
    "features.update({\n",
    "    'yoy_{}'.format(n): yoy(n) for n in range(0, 0)\n",
    "})\n",
    "features.update({\n",
    "    'delta_yoy_{}'.format(n): delta_yoy(n) for n in range(0, 2)\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'willr_{}'.format(n): willr(n) for n in [14, 20, 60]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'linearreg_slope_{}'.format(n): linearreg_slope(n) for n in [14, 112, 224]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    f'kd_{n}': kd(n) for n in [ 9, 14, 20, 60]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'adxr_{}'.format(n): adxr(n) for n in [14, 20, 30]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'ma_ratio_{}'.format(n): ma_ratio(n) for n in [20, 40, 60]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'volume_change_{}'.format(n): volume_change(n) for n in [20, 40, 60]\n",
    "})\n",
    "\n",
    "features.update({\n",
    "    'range_ratio_{}'.format(n): range_ratio(n) for n in [10, 30, 60]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#t1 = pd.read_pickle(\"history/items/bargin_report/外陸資買賣超股數(不含外資自營商).pkl\")\n",
    "#t2 = pd.read_pickle(\"history/items/bargin_report/投信買賣超股數.pkl\")\n",
    "#t3 = pd.read_pickle(\"history/items/bargin_report/自營商買賣超股數(自行買賣).pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 製作dataset\n",
    "\n",
    "##### 設定買賣頻率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2005-02-13', '2005-03-13', '2005-04-13', '2005-05-13',\n",
       "               '2005-06-13', '2005-07-13', '2005-08-13', '2005-09-13',\n",
       "               '2005-10-13', '2005-11-13',\n",
       "               ...\n",
       "               '2023-01-13', '2023-02-13', '2023-03-13', '2023-04-13',\n",
       "               '2023-05-13', '2023-06-13', '2023-07-13', '2023-08-13',\n",
       "               '2023-09-13', '2023-10-13'],\n",
       "              dtype='datetime64[ns]', name='date', length=225, freq=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "every_month = rev.index\n",
    "every_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 將dataframe 組裝起來"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features['bias20'].reindex(every_month, method='ffill')\n",
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.reindex(every_month, method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for name, f in features.items():\n",
    "    features[name] = f.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##我要把金融股拿掉 所以把28開頭全去掉\n",
    "#dataset = dataset[~dataset.index.get_level_values('stock_id').str.startswith('28')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mom1',\n",
       " 'mom2',\n",
       " 'mom3',\n",
       " 'mom4',\n",
       " 'mom5',\n",
       " 'mom6',\n",
       " 'mom7',\n",
       " 'mom8',\n",
       " 'mom9',\n",
       " 'bias_5',\n",
       " 'bias_60',\n",
       " 'bias_120',\n",
       " 'bias_240',\n",
       " 'PB',\n",
       " 'PE',\n",
       " 'DY',\n",
       " 'acc_5',\n",
       " 'acc_10',\n",
       " 'acc_20',\n",
       " 'acc_60',\n",
       " 'acc_120',\n",
       " 'acc_240',\n",
       " 'rsv_60',\n",
       " 'rsv_120',\n",
       " 'rsv_240',\n",
       " 'delta_yoy_0',\n",
       " 'delta_yoy_1',\n",
       " 'willr_14',\n",
       " 'willr_20',\n",
       " 'willr_60',\n",
       " 'linearreg_slope_14',\n",
       " 'linearreg_slope_112',\n",
       " 'linearreg_slope_224',\n",
       " 'kd_9',\n",
       " 'kd_14',\n",
       " 'kd_20',\n",
       " 'kd_60',\n",
       " 'adxr_14',\n",
       " 'adxr_20',\n",
       " 'adxr_30',\n",
       " 'ma_ratio_20',\n",
       " 'ma_ratio_40',\n",
       " 'ma_ratio_60',\n",
       " 'volume_change_20',\n",
       " 'volume_change_40',\n",
       " 'volume_change_60',\n",
       " 'range_ratio_10',\n",
       " 'range_ratio_30',\n",
       " 'range_ratio_60']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = list(dataset.columns)\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_finlab import ml\n",
    "\n",
    "vol=data.get('成交股數')/1000\n",
    "vol_ma5=vol.rolling(20).mean()\n",
    "\n",
    "股本 = data.get('股本合計').reindex(close.index, method='ffill')\n",
    "市值 = 股本 * close / 10 * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml.add_feature(dataset, 'vol_ma5', vol_ma5)\n",
    "\n",
    "ml.add_feature(dataset, '市值', 市值)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 新增 label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finlab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1661485/652783164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinlab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_profit_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_rank_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'finlab'"
     ]
    }
   ],
   "source": [
    "from finlab import ml\n",
    "\n",
    "ml.add_profit_prediction(dataset)\n",
    "ml.add_rank_prediction(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 刪除太大太小的歷史資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)\n",
    "\n",
    "def drop_extreme_case(dataset, feature_names, thresh=0.01):\n",
    "    \n",
    "    extreme_cases = pd.Series(False, index=dataset.index)\n",
    "    for f in feature_names:\n",
    "        tf = dataset[f]\n",
    "        extreme_cases = extreme_cases | (tf < tf.quantile(thresh)) | (tf > tf.quantile(1-thresh))\n",
    "    dataset = dataset[~extreme_cases]\n",
    "    return dataset\n",
    "\n",
    "dataset_drop_extreme_case = drop_extreme_case(dataset,\n",
    "                                              feature_names, thresh=0.01)\n",
    "\n",
    "print(dataset_drop_extreme_case.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna = dataset_drop_extreme_case.dropna(how='any')\n",
    "dataset_dropna = dataset_dropna.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop_extreme_case.index.get_level_values(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop_extreme_case.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_unique = dataset_dropna.index.astype(str).drop_duplicates().sort_values()\n",
    "dataset_unique\n",
    "\n",
    "# 計算要分割的索引位置\n",
    "split_index = int(len(dataset_unique) * 0.9)\n",
    "Bef = dataset_unique[split_index] \n",
    "Aft = dataset_unique[split_index+1]\n",
    "\n",
    "\n",
    "# 分割資料集\n",
    "dataset_train = dataset_dropna.loc[:Bef]\n",
    "dataset_test = dataset_dropna.loc[Aft:]\n",
    "Bef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_train[feature_names].astype(float) , dataset_train['return'] > 1.00\n",
    "test = dataset_test[feature_names].astype(float) , dataset_test['return'] > 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profit(return) rank\n",
    "predi_target = 'rank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神經網路模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "# 建立模型\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.BatchNormalization(input_shape=(len(feature_names),)))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(100, activation='relu',\n",
    "                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "model.add(layers.Dropout(0.4))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 摘要模型\n",
    "model.summary()\n",
    "\n",
    "# 建立優化器\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# 建立損失函數和評估指標\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 設定早停\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "# 設定模型回檔\n",
    "get_best_model = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='./best_model/v2_12.h5',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    save_best_only=True)\n",
    "\n",
    "# 訓練模型\n",
    "history = model.fit(dataset_train[feature_names], dataset_train[predi_target],\n",
    "                    batch_size=888,\n",
    "                    epochs=225,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[get_best_model]) #es, get_best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras import initializers\n",
    "##\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "##\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(layers.Dense(256, activation='relu',\n",
    "#                      input_shape=(len(feature_names),),\n",
    "#                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "#model.add(layers.Dense(128, activation='relu',\n",
    "#                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "#model.add(layers.Dropout(0.4))\n",
    "##model.add(layers.Dense(64, activation='relu',\n",
    "##                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "##model.add(layers.Dense(32, activation='relu',\n",
    "##                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "#model.add(layers.Dense(16, activation='relu',\n",
    "#                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "#model.add(layers.Dropout(0.3))\n",
    "#model.add(layers.Dense(1, activation='sigmoid'))\n",
    "##\n",
    "#model.summary()\n",
    "##\n",
    "#model.compile(loss='mean_squared_error',\n",
    "#              optimizer=\"adam\",)\n",
    "##\n",
    "#print('start fitting')\n",
    "#history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "#                    batch_size=1000,\n",
    "#                    epochs=200,\n",
    "#                    verbose=1,\n",
    "#                    validation_split=0.1)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#\n",
    "#from tensorflow import keras\n",
    "#from tensorflow.keras import layers\n",
    "#from tensorflow.keras import initializers\n",
    "#\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "#\n",
    "#model = keras.models.Sequential()\n",
    "#model.add(layers.Dense(100, activation='relu',\n",
    "#                      input_shape=(len(feature_names),),\n",
    "#                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "#model.add(layers.Dense(100, activation='relu',\n",
    "#                      kernel_initializer=initializers.he_normal(seed=0)))\n",
    "#model.add(layers.Dropout(0.35))\n",
    "#model.add(layers.Dense(1, activation='sigmoid'))\n",
    "#\n",
    "#\n",
    "#model.summary()\n",
    "#\n",
    "#model.compile(loss='mean_squared_error',\n",
    "#              optimizer=\"adam\",)\n",
    "#\n",
    "#print('start fitting')\n",
    "#history = model.fit(dataset_train[feature_names], dataset_train['rank'],\n",
    "#                    batch_size=1000,\n",
    "#                    epochs=225,\n",
    "#                    verbose=1,\n",
    "#                    validation_split=0.1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 訓練過程中的損失函數\n",
    "train_loss = history.history['loss'][5:]\n",
    "val_loss = history.history['val_loss'][5:]\n",
    "\n",
    "# 繪製損失函數圖表\n",
    "plt.plot(range(len(train_loss)), train_loss, label='Training Loss')\n",
    "plt.plot(range(len(val_loss)), val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "fit_params = {\n",
    "    \"early_stopping_rounds\": 30,\n",
    "    \"eval_metric\": 'logloss',  # 使用对数损失作为分类任务的评估指标\n",
    "    \"eval_set\": [(test[0], test[1])],\n",
    "    'eval_names': ['valid'],\n",
    "    'verbose': 100,\n",
    "    'categorical_feature': 'auto'\n",
    "}\n",
    "\n",
    "param_test = {\n",
    "    'num_leaves': sp_randint(10, 100),\n",
    "    'min_child_samples': sp_randint(50, 200),\n",
    "    'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "    'subsample': sp_uniform(loc=0.1, scale=0.9),\n",
    "    'colsample_bytree': sp_uniform(loc=0.2, scale=0.8),\n",
    "    'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "    'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
    "}\n",
    "\n",
    "# 調整為較小的值，例如100\n",
    "n_HP_points_to_test = 100\n",
    "\n",
    "# 将n_estimators设置为10000\n",
    "clf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, n_jobs=4, n_estimators=10000)\n",
    "\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, param_distributions=param_test,\n",
    "    n_iter=n_HP_points_to_test,\n",
    "    scoring='neg_log_loss',  # 使用对数损失作为分类任务的评估指标\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 替換為實際的訓練數據和適合的參數\n",
    "gs.fit(*train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = gs.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"----------------\")\n",
    "gs_best_model = lgb.LGBMRegressor(**best_params )\n",
    "gs_best_model.fit(*train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#Training accuracy {:.4f}'.format(gs_best_model.score(*train)))\n",
    "print('#Testing accuracy {:.4f}'.format(gs_best_model.score(*test)))\n",
    "\n",
    "#l1那個\n",
    "#Training accuracy 0.1127\n",
    "#Testing accuracy -0.0138\n",
    "\n",
    "#Training accuracy 0.0581\n",
    "#Testing accuracy -0.0043\n",
    "\n",
    "#Training accuracy 0.1361\n",
    "#Testing accuracy -0.0167\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(gs_best_model.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "cf_xgb = xgb.XGBClassifier()  # 使用XGBClassifier作为分类器\n",
    "cf_xgb.fit(*train)\n",
    "accuracy = cf_xgb.score(*test)  # 计算分类准确率等评估指标\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "clf = xgb.XGBClassifier(objective='binary:logistic')  # 使用XGBClassifier作为分类器，设置分类的目标函数\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [6, 10, 15, 20],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n",
    "    'gamma': [0, 0.25, 0.5, 1.0],\n",
    "    'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n",
    "    'n_estimators': [100]\n",
    "}\n",
    "\n",
    "fit_params = {\n",
    "    'early_stopping_rounds': 10,\n",
    "    'eval_set': [(test[0], test[1])],\n",
    "    'verbose': False,\n",
    "    'eval_metric': 'logloss'  # 使用对数损失作为分类任务的评估指标\n",
    "}\n",
    "\n",
    "rs = RandomizedSearchCV(clf, param_grid, n_iter=20, n_jobs=1, verbose=2, cv=2, scoring='neg_log_loss', refit=True, random_state=42)\n",
    "rs.fit(train[0], train[1], **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_params = rs.best_params_\n",
    "print(\"Best Parameters:\", xgb_best_params)\n",
    "print(\"----------------\")\n",
    "xgb_best_model = xgb.XGBClassifier(**best_params )\n",
    "xgb_best_model.fit(*train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_best_model.score(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(zip(xgb_best_model.feature_importances_, feature_names), \n",
    "                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "feature_imp\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#\n",
    "#cf2 = RandomForestRegressor(n_estimators=100)\n",
    "#cf2.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.model_selection import RandomizedSearchCV\n",
    "#\n",
    "## 建立隨機森林回歸模型\n",
    "#cf2 = RandomForestRegressor()\n",
    "#\n",
    "## 定義超參數範圍\n",
    "#param_dist = {\n",
    "#    'n_estimators': [50, 100, 200, 300],\n",
    "#    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "#    'max_depth': [None, 10, 20, 30],\n",
    "#    'min_samples_split': [2, 5, 10],\n",
    "#    'min_samples_leaf': [1, 2, 4],\n",
    "#    'bootstrap': [True, False]\n",
    "#}\n",
    "#\n",
    "## 執行隨機參數搜尋\n",
    "#rs = RandomizedSearchCV(cf2, param_distributions=param_dist, n_iter=100, cv=5, verbose=2, random_state=42)\n",
    "#rs.fit(dataset_train[feature_names].astype(float), dataset_train['rank'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上程式碼中，我們建立了 RandomForestRegressor 模型，\n",
    "並定義了超參數的範圍。使用 RandomizedSearchCV 進行參數搜尋，\n",
    "設定了迭代次數為 100，交叉驗證次數為 5，並設定了 random_state 來保持結果的可重現性。\n",
    "透過這樣的方式，您可以進一步優化 RandomForestRegressor 模型，以提高其性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#feature_imp = pd.DataFrame(zip(cf2.feature_importances_, feature_names), \n",
    "#                           columns=['Value','Feature']).sort_values('Value', ascending=False)\n",
    "#feature_imp\n",
    "#\n",
    "#%matplotlib inline\n",
    "#import seaborn as sns\n",
    "#plt.figure(figsize=(10,10))\n",
    "#sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_drop = dataset.dropna(subset=feature_names+['return'])\n",
    "\n",
    "vals = model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result1'] = pd.Series(vals.swapaxes(0,1)[0], dataset_drop.index)\n",
    "\n",
    "vals = gs_best_model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result2'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "vals = xgb_best_model.predict(dataset_drop[feature_names].astype(float))\n",
    "dataset_drop['result3'] = pd.Series(vals, dataset_drop.index)\n",
    "\n",
    "dataset_drop = dataset_drop.reset_index().set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "dates = sorted(list(set(dataset_drop.index)))\n",
    "fil_ma_s = 1000  # start\n",
    "fil_ma_e = 3000  # end\n",
    "\n",
    "rs_1 = []\n",
    "rs_2 = []\n",
    "rs_3 = []  \n",
    "rs_4 = []  \n",
    "rs_5 = []  # 收益数据列表\n",
    "\n",
    "for d in tqdm(dates):\n",
    "    dataset_time = dataset_drop.loc[d]\n",
    "    dataset_time = drop_extreme_case(dataset_time, feature_names, thresh=0.01)\n",
    "    predi_target_0 = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3']\n",
    "    predi_target_1 = predi_target_0.where((dataset_time['vol_ma5'] > fil_ma_s) & (dataset_time['vol_ma5'] < fil_ma_e) & (dataset_time['市值'] < 1e10), 0)\n",
    "    predi_target_2 = predi_target_0.where((dataset_time['vol_ma5'] > fil_ma_s) & (dataset_time['市值'] < 1e9), 0)\n",
    "    predi_target_3 = predi_target_0.where((dataset_time['vol_ma5'] < fil_ma_e) & (dataset_time['市值'] < 1e9), 0)\n",
    "    predi_target_4 = predi_target_0.where((dataset_time['vol_ma5'] > fil_ma_e) & (dataset_time['市值'] > 1e9), 0)\n",
    "    \n",
    "        \n",
    "\n",
    "    condition_0 = (predi_target_0 >= predi_target_0.nlargest(20).iloc[-1])\n",
    "    condition_1 = (predi_target_1 >= predi_target_1.nlargest(20).iloc[-1])\n",
    "    condition_2 = (predi_target_2 >= predi_target_2.nlargest(20).iloc[-1])\n",
    "    condition_3 = (predi_target_3 >= predi_target_3.nlargest(20).iloc[-1])\n",
    "    condition_4 = (predi_target_4 >= predi_target_4.nlargest(20).iloc[-1])\n",
    "\n",
    "    \n",
    "    r0 = dataset_time['return'][condition_0].mean()\n",
    "    r1 = dataset_time['return'][condition_1].mean()\n",
    "    r2 = dataset_time['return'][condition_2].mean()\n",
    "    r3 = dataset_time['return'][condition_3].mean()\n",
    "    r4 = dataset_time['return'][condition_4].mean()\n",
    "\n",
    "    \n",
    "    rs_1.append(r0 * (1 - 3 / 1000 - 1.425 / 1000 * 2 * 0.6))\n",
    "    rs_2.append(r1 * (1 - 3 / 1000 - 1.425 / 1000 * 2 * 0.6))\n",
    "    rs_3.append(r2 * (1 - 3 / 1000 - 1.425 / 1000 * 2 * 0.6))\n",
    "    rs_4.append(r3 * (1 - 3 / 1000 - 1.425 / 1000 * 2 * 0.6))\n",
    "    rs_5.append(r4 * (1 - 3 / 1000 - 1.425 / 1000 * 2 * 0.6))\n",
    "    \n",
    "\n",
    "rs_1 = pd.Series(rs_1, index=dates)[Aft:].cumprod()\n",
    "rs_2 = pd.Series(rs_2, index=dates)[Aft:].cumprod()\n",
    "rs_3 = pd.Series(rs_3, index=dates)[Aft:].cumprod()\n",
    "rs_4 = pd.Series(rs_4, index=dates)[Aft:].cumprod()\n",
    "rs_5 = pd.Series(rs_5, index=dates)[Aft:].cumprod()\n",
    "\n",
    "s0050 = close['0050'][Aft:]\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        'nn strategy return': rs_1.reindex(s0050.index, method='ffill'),\n",
    "        'nn strategy return_ma': rs_2.reindex(s0050.index, method='ffill'),\n",
    "        'nn strategy return_mktcap->100 10e': rs_3.reindex(s0050.index, method='ffill'),\n",
    "        #'nn strategy return_mktcap-<300 10e': rs_4.reindex(s0050.index, method='ffill'),\n",
    "        'nn strategy return_mktcap->300 10e': rs_5.reindex(s0050.index, method='ffill'),\n",
    "        '0050 return': s0050 / s0050[0],\n",
    "    }\n",
    ").plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predi_target = dataset_time['result1'] + dataset_time['result2'] + dataset_time['result3']\n",
    "predi_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 當月持股狀況"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.levels[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the latest dataset\n",
    "last_date = dataset.index.levels[1].max()#\"2022-10-15\"\n",
    "is_last_date = dataset.index.get_level_values('date') == last_date\n",
    "last_dataset = dataset[is_last_date].copy()\n",
    "\n",
    "\n",
    "last_dataset = drop_extreme_case(last_dataset,\n",
    "                                 feature_names, thresh=0.01)\n",
    "\n",
    "\n",
    "# remove NaN testcases\n",
    "last_dataset = last_dataset.dropna(subset=feature_names)\n",
    "\n",
    "# predict\n",
    "\n",
    "vals = model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result1'] = pd.Series(vals.swapaxes(0,1)[0], last_dataset.index)\n",
    "\n",
    "vals = gs_best_model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result2'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "vals = xgb_best_model.predict(last_dataset[feature_names].astype(float))\n",
    "last_dataset['result3'] = pd.Series(vals, last_dataset.index)\n",
    "\n",
    "# calculate score\n",
    "\n",
    "rank = last_dataset['result1'] + last_dataset['result2'] + last_dataset['result3']\n",
    "#rank_target = rank.where((last_dataset['vol_ma5'] > fil_ma_s)  & (last_dataset['vol_ma5'] < fil_ma_e)  & (last_dataset['市值'] < 1e10), 0)\n",
    "rank_target = rank.where((last_dataset['vol_ma5'] > fil_ma_e)  & (last_dataset['市值'] < 1e10), 0)\n",
    "\n",
    "condition = (rank_target >= rank_target.nlargest(20).iloc[-1]) \n",
    "\n",
    "# plot rank distribution\n",
    "rank_target[rank_target != 0].hist(bins=20)\n",
    "\n",
    "\n",
    "# show the best 20 stocks\n",
    "slist1 = rank_target[rank_target != 0].reset_index()['stock_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank_target['8299']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 平均分配資產於股票之中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = data.get(\"收盤價\")\n",
    "\n",
    "money = 662919\n",
    "stock_prices = close[rank_target[condition].reset_index()['stock_id']].iloc[-1]\n",
    "\n",
    "\n",
    "print(\"股票平分張數:\")\n",
    "money / len(stock_prices) / stock_prices / 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_stock = '1101'\n",
    "(last_dataset['vol_ma5'][T_stock] > fil_ma_s)\n",
    "(last_dataset['vol_ma5'][T_stock] < fil_ma_e)  & (last_dataset['市值'] < 1e10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfolio as pf\n",
    "\n",
    "import pickle\n",
    "pickle.dump(rs_2, open('230618_return_history.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " rs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pf.tears.create_capacity_tear_sheet(rs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 得到 上面的 回測結果\n",
    "ret = pickle.load(open(\"230618_return_history.pkl\", \"rb\"))\n",
    "\n",
    "# 將回測報酬率取出來\n",
    "ret = ret.pct_change().dropna()\n",
    "#ret.index = pd.to_datetime(ret.index).tz_localize('Asia/Taipei')\n",
    "\n",
    "# 利用pyfolio 比較報酬率\n",
    "\n",
    "pf.create_returns_tear_sheet(ret, benchmark_rets=close['0050'].reindex(ret.index, method='ffill').pct_change())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################################\n",
    "## 移動窗格最佳化\n",
    "[有空來試試看連結](https://hahow.in/courses/5b9d3a6dca498a001e917383/discussions/61b4c90147843d0006cf2593)\n",
    "\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset_train[feature_names].astype(float) , dataset_train['return'] #> 1.00\n",
    "test = dataset_test[feature_names].astype(float) , dataset_test['return'] #> 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 創建Random Forest模型\n",
    "random_forest = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# 使用訓練資料訓練模型\n",
    "random_forest.fit(*train)\n",
    "\n",
    "# 使用測試資料評估模型\n",
    "accuracy = random_forest.score(*test)\n",
    "print(\"Random Forest 模型的準確率：\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def select(df):\n",
    "\n",
    "    rank = df['pre']\n",
    "\n",
    "    condition1 = (rank >= rank.nlargest(1).iloc[-1])\n",
    "\n",
    "    return df['return'][condition1].mean() * (1-3/1000-1.425/1000*2*0.6)\n",
    "\n",
    "end = 5\n",
    "\n",
    "cf = lgb.LGBMRegressor(n_estimators=500)\n",
    "\n",
    "\n",
    "\n",
    "train_time = ['2015','2016','2017','2018','2019']\n",
    "\n",
    "s_time = ['2007','2008','2009','2010','2011']\n",
    "\n",
    "test_time = ['2016','2017','2018','2019','2020']\n",
    "\n",
    "dataset_copy = dataset_dropna.copy()\n",
    "\n",
    "store_mse = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in range(end):\n",
    "\n",
    "    print('%d 次執行中'%(time+1))\n",
    "\n",
    "    dataset_dropna2_train = dataset_copy.loc[s_time[time]:train_time[time]] #2007~ 2015   2008~2016   2009~2017  2010~2018  ....\n",
    "\n",
    "    dataset_dropna2_test = dataset_copy.loc[test_time[time]:test_time[time]]#            2016                2017               2018              2019  .....\n",
    "\n",
    "    \n",
    "    cf.fit(dataset_dropna2_train[feature_names].astype(float), dataset_dropna2_train['rank'])\n",
    "    predict = cf.predict(dataset_dropna2_test[feature_names])\n",
    "\n",
    "    dataset_dropna2_test['pre'] = predict\n",
    "\n",
    "    dates = dataset_dropna2_test.index.get_level_values('date')\n",
    "    \n",
    "    b = dataset_dropna2_test.groupby(dates).apply(select).cumprod()\n",
    "    \n",
    "    s0050 = close['0050'][test_time[time]:test_time[time]]\n",
    "    \n",
    "    s0056 = close['0056'][test_time[time]:test_time[time]]\n",
    "    \n",
    "    pd.DataFrame({'Best 1 stocks return(include handling fee)':b.reindex(s0050.index, method='ffill'), \n",
    "    \n",
    "                  '0050':s0050/s0050[0],'0056':s0056/s0056[0]}).plot()\n",
    "    \n",
    "    plt.ylabel('return')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.predict(dataset_dropna2_test[feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dropna2_train[feature_names].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finlab",
   "language": "python",
   "name": "finlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
